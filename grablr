#!/bin/bash

 # ==================================================== #
 # This script looks better in a maximized window, on   #
 # 1360px or wider screens...                           #
 #                                                      #
 # Run it with the '-h' option for better understanding #
 #                                                      #
 # -- - - - - - - - - - - - - - - - - - - - - - - - - - #
 # Originally written by <pedrovernetti@gmail.com>      #
 # -- - - - - - - - - - - - - - - - - - - - - - - - - - #
 # tested in / thought for Ubuntu 16.04 and 18.04       #
 # ==================================================== #

# HELP ------------------------------------------------------------------------
help_message ()
{
    printf "Usage: ${0##*/} [OPTIONS] BLOG_URL DOWNLOAD_FOLDER\n\n"
    printf "  Downloads every relevant (>2kB) image from a given blog.\n\n"
    if [[ "$1" == "full" ]]; then
        printf "  Options:\n\n"
        printf "  -h        print this help message and exit\n"
        printf "  -prepare  install dependencies (\033[1mmust be runned once\033[0m)\n"
        printf "  -e \033[2mN\033[0m      finish grabbing at blog's \033[2mN\033[0mth page \n"
        printf "  -M \033[2mSIZE\033[0m   do not download images smaller than 'SIZE' bytes\n"
        printf "  -O        run jpegoptim on downloaded JPEGs\n"
        printf "  -P        images of each page are downloaded to corresponding subfolders\n"
        printf "  -s \033[2mN\033[0m      start grabbing from blog's \033[2mN\033[0mth page \n\n"
        printf "  Currently supported platforms include only Tumblr.\n\n"
        exit 0
    else
        exit 1
        fi
}

# INSTALLING DEPENDENCIES IF DEMANDED -----------------------------------------
install_dependencies ()
{
    sudo apt-get -y install wget
    sudo apt-get -y install grep
    #sudo apt-get -y install imagemagick
    sudo apt-get -y install jpegoptim
    exit
}
page_url_structure ()
{
    if wget -q --spider "$1/page/1" &> /dev/null; then echo "$1/page/"
    elif wget -q --spider "$1/1" &> /dev/null; then echo "$1/"
    elif wget -q --spider "$1/new/1" &> /dev/null; then echo "$1/new/"
    elif wget -q --spider "$1&page" &> /dev/null; then echo "$1&"
    else
        echo "could not guess URL structure for '$1'"
        exit 90
        fi
}

# DEFINING UTILITY FUNCTIONS --------------------------------------------------
seems_relevant_by_size ()
{
    if [[ "$MINIMUM_SIZE_FILTER" == false ]]; then return 0; fi
    size=$(wget --spider --server-response "$1" 2>&1 | \
            grep -Eio '^[\t ]*Content-Length: [[:digit:]]+' | \
            grep -Eo '[[:digit:]]+')
    if [[ "$size" -lt $MINSIZE ]]; then return 1
    else return 0; fi
}

# SETTING DEFAULT VALUES ------------------------------------------------------
separator="- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -"
tld_rx='(com?|org|net)(\.(br|u[ks]))?'
blog_name_rx='[[:alnum:]_-]+'
blog_host_rx='tumblr'
blog_url_rx="$blog_name_rx\.$blog_host_rx\.$tld_rx"
img_link_rx='src=['\''"]https?://.*?\.(jp(e?g|e)|png|gif|webp)['\''"]'
FIRST_PAGE=1
LAST_PAGE=999999
SUBFOLDERS=false
JPEGOPTIM=false
MINIMUM_SIZE_FILTER=false
MINSIZE=0

# DEALING WITH COMMAND LINE ARGUMENTS -----------------------------------------
ARG_A="${@:(-2):1}"
ARG_A="${ARG_A##*://}"
ARG_A="http://${ARG_A%%\/*}"
ARG_B="${!#}"
ARG_B="${ARG_B%\/}"
for i in `seq 1 $#`; do
    if [[ "${!i}" =~ ^-(h|-?help)$ ]]; then help_message full; fi
    if [[ "${!i}" =~ ^--?prepare$ ]]; then install_dependencies; fi
    if [[ "${!i}" =~ ^-(e|-?finish-at-page)$ ]]; then
        i=$((i + 1))
        LAST_PAGE="${!i}"
        fi
    if [[ "${!i}" =~ ^-(M|-?minimum-size)$ ]]; then
        MINIMUM_SIZE_FILTER=true
        i=$((i + 1))
        MINSIZE="${!i}"
        fi
    if [[ "${!i}" =~ ^-(O|-?jpegoptim)$ ]]; then JPEGOPTIM=true; fi
    if [[ "${!i}" =~ ^-(P|-?create-subfolders)$ ]]; then SUBFOLDERS=true; fi
    if [[ "${!i}" =~ ^-(s|-?start-at-page)$ ]]; then
        i=$((i + 1))
        FIRST_PAGE="${!i}"
        fi
    done
if ! [[ "$MINSIZE" =~ ^[\t\ ]*[[:digit:]]+[\t\ ]*$ ]]; then
    echo "'$MINSIZE' is not a valid file size"
    exit 96
elif ! [[ "$FIRST_PAGE" =~ ^[[:digit:]]{1,6}$ ]]; then
    echo "'$FIRST_PAGE' is not a valid page number"
    exit 97
elif ! [[ "$LAST_PAGE" =~ ^[[:digit:]]{1,6}$ ]]; then
    echo "'$LAST_PAGE' is not a valid page number"
    exit 97
elif [[ ( $# -eq 0 ) || ( "$ARG_A" == "$0" ) ]]; then
    help_message short
elif ! [[ -d "$ARG_B" ]]; then
    echo "'$ARG_B' is not a valid folder"
    exit 98
elif ! [[ "$ARG_A" =~ ^https?://$blog_url_rx/?$ ]]; then
    echo "'$ARG_A' is not a valid blog URL"
    exit 99
else
    BLOG="$ARG_A"
    DOWNLOAD_FOLDER="$ARG_B"
    HTML=$(echo "$BLOG" | grep -Eo "$blog_name_rx\.$blog_host_rx")
    HTML="/tmp/$HTML.page"
    fi

# DOWNLOADING PAGES AND THEN THE IMAGES REFERENCED THEREIN --------------------
i=$FIRST_PAGE
printf "\n\033[1;4;35m${BLOG##*://}\033[0m\n\033[35m$separator\033[0m\n"
BLOG=$(page_url_structure "$BLOG")
while wget -q -c -O "$HTML$i.html" "$BLOG$i" && [[ "$i" -le "$LAST_PAGE" ]]; do
    if [[ "$SUBFOLDERS" == true ]]; then mkdir "$DOWNLOAD_FOLDER/$i"; fi
    printf "\n\033[1mPAGE $i\033[0m\n\n"
    while read -r img_url; do
        img_url="${img_url#src=?}"
        img_url="${img_url%?}"
        if seems_relevant_by_size $img_url; then
            img_path="${img_url##*/}"
            if [[ "$SUBFOLDERS" == true ]]; then img_path="$i/$img_path"; fi
            img_path="$DOWNLOAD_FOLDER/$img_path"
            echo "Downloading '$img_url'"
            wget -q -c -O "$img_path" "$img_url"
            if [[ ( "$JPEGOPTIM" == true ) && \
                  ( "${img_path,,}" =~ ^.*jp(e?g|e)$ ) ]]; then
                  jpegoptim -q -p --strip-iptc --strip-icc "$img_path"
                  fi
            fi
        done <<< $(grep -Eio "$img_link_rx" "$HTML$i.html")
    rm -fr "$HTML$i.html" &> /dev/null
    i=$((i+1))
    done


